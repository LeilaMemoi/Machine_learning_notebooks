{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LAB SESSION 2 - Bagging and Random Forests**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: in all this project, use **the value 0** when you need to choose a value for a random state*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 1: comparison between random forests and bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We work with the [Urban Land Cover data base](https://archive.ics.uci.edu/ml/datasets/Urban+Land+Cover). The data are used for automated mapping of urban land cover (trees, grass, soil, concrete, asphalt, buildings, etc.) in satellite or aerial imagery. Nine types of urban land cover are considered and multi-scale spectral, size, shape, and texture information are used for classification. The data consists in a train set and a test set. **The goal is to predict the urban land cover (the variable named `class`) based on the multi-scale spectral, size, shape, and texture information. It is then a classification problem. We will use the overall accuracy (1-misclassification rate) as performance criterion**. Note that other preformnce criterion exist for classification problem such as specificity, sensitivity, F-score, etc.\n",
    "\n",
    "Before to start, we: \n",
    " 1) load the data and perform a briel descriptive analysis of them;\n",
    " 2) select the variables that we will use in the exercise, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data \n",
    "import pandas as pd # data analysis\n",
    "ulc_train = pd.read_csv(\"ULC_training.csv\") \n",
    "ulc_test = pd.read_csv(\"ULC_testing.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the dimension\n",
    "print(ulc_train.shape)\n",
    "print(ulc_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a first quick look at the datasets and display the dimension\n",
    "ulc_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ulc_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display a statistic summary for the numerical variables\n",
    "ulc_train.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ulc_test.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display the distribution of the target variable in the two datasets\n",
    "ulc_train['class'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ulc_test['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the variables that we be used \n",
    "target_column = \"class\" # The response variable that we will consider\n",
    "features_columns = list(ulc_train)\n",
    "features_columns.remove('class') # The predictors/features used to predict the target\n",
    "#print(target_column)\n",
    "#print(features_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target = ulc_train[features_columns], ulc_train[target_column]\n",
    "data_test, target_test = ulc_test[features_columns], ulc_test[target_column]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1. Fit a random forest named *`rfc`* on the train set to explain the type of urban land cover (variable `class`) according to multi-scale spectral, size, shape, and texture information. More specifically, you will use the [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) class. Read carefully the documentation.\n",
    "\n",
    "*Indication: for the hyperparameters, use the values `n_estimators = 500` and `max_features= sqrt(d)` with `d` denoting the number of features, `oob_score=True`  and `random_state=0`.*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before buiding `rfc` remind in the window below the meaning of`n_estimators` and `max_features`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer:............."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##------- Complete the command below by filling in the gaps '...'.-------##\n",
    "\n",
    "# Step 1: create the object rfc, it is a RandomForestClassifier object with n_estimators=500, max_features='auto' and random_state=0\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(......)\n",
    "\n",
    "# Step 2: build the random forest on the train set by indicating the input data and the target variable \n",
    "rfc.fit(.....)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solution\n",
    "\n",
    "# Step 1: create the object rfc, it is a RandomForestClassifier object with n_estimators=500, max_features='auto' and random_state=0\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators=500, max_features='sqrt',oob_score=True,random_state=0)\n",
    "\n",
    "# Step 2: build the random forest on the train set by indicating the input data and the target variable \n",
    "rfc.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##------- Complete the command below by filling in the gaps '...'.-------##\n",
    "\n",
    "# Step 3: look at the parameters used by your forest\n",
    "from pprint import pprint\n",
    "print('Parameters of the forest:\\n')\n",
    "pprint(rfc.get_params())\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the oob error and comment the result. What does it represent ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##------- Complete the command below by filling in the gaps '...'.-------##\n",
    "# Step 4: print the oob_score (attributes of rfc named oob_score_)\n",
    "....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: print the oob_score (attributes of rfc named oob_score_)\n",
    "print('OOB error:'),\n",
    "print(rfc.oob_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meaning of the OOB error and result interpretation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Comment: ............."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2. Predict the class of each observation of the test sample by using the random forest `rfc`and display the confusion matrix. Comment it. How many observations are misclassified ? Compute the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##------- Complete the command below by filling in the gaps '...'.-------##\n",
    "\n",
    "# Step 1: compute the predictions on the test set\n",
    "predictions_test = rfc.predict(.......) \n",
    "\n",
    "# Step 2: display the confusion matrix on the test set\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "cm = confusion_matrix(....., .......)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=.....,display_labels=rfc.classes_)\n",
    "disp.plot() \n",
    "\n",
    "# Step 3: compute the accuracy on the test set\n",
    "accuracy = rfc.score(....,.....)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solution\n",
    "\n",
    "# Step 1: compute the predictions\n",
    "predictions_test = rfc.predict(data_test) \n",
    "\n",
    "# Step 2: display the confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "cm = confusion_matrix(target_test, predictions_test)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=rfc.classes_)\n",
    "disp.plot() \n",
    "\n",
    "# Step 3: compute the accuracy\n",
    "accuracy = rfc.score(data_test,target_test)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solution\n",
    "\n",
    "# Step 1: compute the predictions\n",
    "predictions_test = rfc.predict(data_test) \n",
    "\n",
    "# Step 2: display the confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "cm = confusion_matrix(target_test, predictions_test)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=rfc.classes_)\n",
    "disp.plot() \n",
    "\n",
    "# Step 3: compute the accuracy\n",
    "accuracy = rfc.score(data_test,target_test)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment: ........."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of misclassified observations in the test set:............"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3. We will now focus on the calibration of some RF parameters: `n_estimators` and `max_features`. To calibrate these parameters, we will used the OOB errors. The code below shows how the OOB error can be measured at the addition of each new tree during training. The resulting plot can be used to approximate a suitable value of `n_estimators` at which the OOB error stabilizes. Comment this plot. What value for `n_estimators` does it seem suitable ?\n",
    "\n",
    "*Indication: a suitable value for `n_estimators` is a value for which the oob error of the forest is stable.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RANDOM_STATE = 0\n",
    "\n",
    "\n",
    "# Map a classifier name to a list of (<n_estimators>, <error rate>) pairs.\n",
    "error_rate = []\n",
    "\n",
    "# Range of `n_estimators` values to explore.\n",
    "min_estimators = 100\n",
    "max_estimators = 800\n",
    "step=5\n",
    "\n",
    "for i in range(min_estimators, max_estimators+1, step):\n",
    "    rf = RandomForestClassifier(warm_start=True, n_estimators=i, max_features='auto',random_state=RANDOM_STATE, oob_score=True)\n",
    "    rf.fit(data, target)\n",
    "\n",
    "    # Record the OOB error for each `n_estimators=i` setting.\n",
    "    oob_error = 1 - rf.oob_score_\n",
    "    error_rate.append(oob_error)\n",
    "  \n",
    "\n",
    "# Plot Generate the \"OOB error rate\" vs. \"n_estimators\" plot\n",
    "\n",
    "plt.plot(range(min_estimators, max_estimators +1, step), error_rate, label=\"OOB error rate\")\n",
    "\n",
    "plt.ylim(0, 1.5*max(error_rate)) \n",
    "plt.xlim(min_estimators, max_estimators)\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylabel(\"OOB error rate vs. number of trees\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RANDOM_STATE = 0 # Note: if you modifiy the random stat, the results can be sligthly different\n",
    "\n",
    "\n",
    "# Map a classifier name to a list of (<n_estimators>, <error rate>) pairs.\n",
    "error_rate = []\n",
    "\n",
    "# Range of `n_estimators` values to explore.\n",
    "min_estimators = 100\n",
    "max_estimators = 800\n",
    "step=5\n",
    "\n",
    "for i in range(min_estimators, max_estimators+1, step):\n",
    "    rf = RandomForestClassifier(warm_start=True, n_estimators=i, max_features='auto',random_state=RANDOM_STATE, oob_score=True)\n",
    "    rf.fit(data, target)\n",
    "\n",
    "    # Record the OOB error for each `n_estimators=i` setting.\n",
    "    oob_error = 1 - rf.oob_score_\n",
    "    error_rate.append(oob_error)\n",
    "  \n",
    "\n",
    "# Plot Generate the \"OOB error rate\" vs. \"n_estimators\" plot\n",
    "\n",
    "plt.plot(range(min_estimators, max_estimators +1, step), error_rate, label=\"OOB error rate\")\n",
    "\n",
    "plt.ylim(0, 1.5*max(error_rate)) \n",
    "plt.xlim(min_estimators, max_estimators)\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylabel(\"OOB error rate vs. number of trees\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment of the plot and choice of a suitable value for `n_estimators`: .............\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4. Now, we will repeat `B` times a `k`-fold cross validation with the function GridSearchCV to calibrate at the same time the parameters `n_estimators` and `max_features`. Complete the command below, comment the results and select suitable values for the parameters `n_estimators` and `max_features`.\n",
    "\n",
    "*Indications:*\n",
    "- *the values considers for `n_estimators` and `max_features` are `max_features = (0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9)` and `n_estimators=(400,600,800)`. The values (0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9) for `max_features` represent the proportions of selected features. See the documention of [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) class.*\n",
    "- *we will repeat `B=10`times a k-fold crossvalidation with `k=3`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##------- Complete the command below by filling in the gaps '...'.-------##\n",
    "\n",
    "# Step 1: create a grid with all the values that we will considers for the two paramters\n",
    "grid = {\n",
    "    'max_features':[..........], \n",
    "    'n_estimators':[...........]\n",
    "}\n",
    "\n",
    "# Step 2: use the grid to to search for the best couple of parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "rf = RandomForestClassifier() # create the forest model to tune\n",
    "\n",
    "B=.......\n",
    "results_cv=pd.DataFrame()\n",
    "for i in range(B):\n",
    "    \n",
    "    rf_cv = GridSearchCV(estimator=rf,param_grid=grid, cv=......,n_jobs=-1)# Search the best values for the parameters using 3-fold cross validation, and use all available cores(n_jobs=-1)\n",
    "    rf_cv.fit(data, target) # Fit the CV search\n",
    "    if i==0: \n",
    "        results_cv=pd.DataFrame(rf_cv.cv_results_)[[\"params\",\"mean_test_score\",\"std_test_score\"]]\n",
    "    else:\n",
    "        results_cv[\"mean_test_score\"]=results_cv[\"mean_test_score\"]+pd.DataFrame(rf_cv.cv_results_)[\"mean_test_score\"]\n",
    "        results_cv[\"std_test_score\"]=results_cv[\"std_test_score\"]+pd.DataFrame(rf_cv.cv_results_)[\"std_test_score\"]  \n",
    "        \n",
    "        \n",
    "\n",
    "results_cv[\"mean_test_score\"]=results_cv[\"mean_test_score\"]/B\n",
    "results_cv[\"std_test_score\"]=results_cv[\"std_test_score\"]/B \n",
    "\n",
    "\n",
    "# Step 3: get the best parameters (with the higher performance)\n",
    "ind_best=results_cv[\"mean_test_score\"].idxmax()\n",
    "print(results_cv[\"params\"].iloc[ind_best])\n",
    "print(results_cv[\"mean_test_score\"].iloc[ind_best])\n",
    "print(results_cv[\"std_test_score\"].iloc[ind_best])\n",
    "print(results_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##------- Complete the command below by filling in the gaps '...'.-------##\n",
    "\n",
    "# Step 1: create a grid with all the values that we will considers for the two paramters\n",
    "grid = {\n",
    "    'max_features':[..........], \n",
    "    'n_estimators':[...........]\n",
    "}\n",
    "\n",
    "# Step 2: use the grid to to search for the best couple of parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "rf = RandomForestClassifier() # create the forest model to tune\n",
    "\n",
    "B=.......\n",
    "results_cv=pd.DataFrame()\n",
    "for i in range(B):\n",
    "    \n",
    "    rf_cv = GridSearchCV(estimator=rf,param_grid=grid, cv=......,n_jobs=-1)# Search the best values for the parameters using 3-fold cross validation, and use all available cores(n_jobs=-1)\n",
    "    rf_cv.fit(data, target) # Fit the CV search\n",
    "    if i==0: \n",
    "        results_cv=pd.DataFrame(rf_cv.cv_results_)[[\"params\",\"mean_test_score\",\"std_test_score\"]]\n",
    "    else:\n",
    "        results_cv[\"mean_test_score\"]=results_cv[\"mean_test_score\"]+pd.DataFrame(rf_cv.cv_results_)[\"mean_test_score\"]\n",
    "        results_cv[\"std_test_score\"]=results_cv[\"std_test_score\"]+pd.DataFrame(rf_cv.cv_results_)[\"std_test_score\"]  \n",
    "        \n",
    "        \n",
    "\n",
    "results_cv[\"mean_test_score\"]=results_cv[\"mean_test_score\"]/B\n",
    "results_cv[\"std_test_score\"]=results_cv[\"std_test_score\"]/B \n",
    "\n",
    "\n",
    "# Step 3: get the best parameters (with the higher performance)\n",
    "ind_best=results_cv[\"mean_test_score\"].idxmax()\n",
    "print(results_cv[\"params\"].iloc[ind_best])\n",
    "print(results_cv[\"mean_test_score\"].iloc[ind_best])\n",
    "print(results_cv[\"std_test_score\"].iloc[ind_best])\n",
    "print(results_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result interpretation and choice of tuned values for `n_estimators` and `max_features`: ........"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution for the interpretation and the selection of values for `n_estimators` and `max_features`: ......."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 5. Build the random forest `opt_rfc` by using the best values for the parameters `n_estimators` and `max_features`. Compute the accuracy on the test set and display the confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##------- Complete the command below by filling in the gaps '...'.-------##\n",
    "\n",
    "# Step 1: fit a random forest with the best values for parameters\n",
    "opt_rfc = RandomForestClassifier(n_estimators=...., max_features=....,oob_score=True,random_state=0)\n",
    "\n",
    "# Step 2: build the random forest on the train set by indicating the input data and the target variable \n",
    "opt_rfc.fit(...,.....)\n",
    "\n",
    "# Step 3: compute the accuracy and the confusion matrix\n",
    "predictions_test_2 = opt_rfc.predict(....) \n",
    "cm_2 = confusion_matrix(...., .....)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_2,display_labels=opt_rfc.classes_)\n",
    "disp.plot() \n",
    "\n",
    "# Step 4: compute the accuracy\n",
    "accuracy_2 = opt_rfc.score(....,.....)\n",
    "print(accuracy_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 6. what value for`max_features` we have to choose if we want to apply the bagging algorithm with CART instead of a random forest ? Build this model and compute the prediction error of this model based on the test set. We will call this model $bag$.\n",
    "\n",
    "*Indication: use the value selected at `question 5`for the parameter `n_estimators`.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##------- Complete the command below by filling in the gaps '...'.-------##\n",
    "\n",
    "# Step 1: fit a a bagging model (use the best value for n_estimators)\n",
    "bag = RandomForestClassifier(n_estimators=...., max_features=....,oob_score=True,random_state=0)\n",
    "\n",
    "# Step 2: build the random forest on the train set by indicating the input data and the target variable \n",
    "bag.fit(...., ......)\n",
    "\n",
    "# Step 3: compute the prediction on the test set and the confusion matrix\n",
    "predictions_test_bag = bag.predict(......) \n",
    "cm_bag = confusion_matrix(.....,......)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_bag,display_labels=bag.classes_)\n",
    "disp.plot() \n",
    "\n",
    "# Step 4: compute the accuracy on the test set\n",
    "accuracy_bag = bag.score(....,.....)\n",
    "print(accuracy_bag)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 8. Compare your three models `opt_rfc`, `bag`and `rfc` using suitable performance criteria. What model do you choose and why ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your Answer: ........\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 9. The code below shows the boxplot of the mean decrease in accuracy (MDA) for the 15 features with the highest average MDA. The MDA of each of the 147 features has been independently computed `n__repeats =10` times on the test set for the random forest `opt_rfc` (at each run, another permutation is applied for the features). Comment the plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "result = permutation_importance(opt_rfc, data_test, target_test, n_repeats=10, random_state=0, n_jobs=2)\n",
    "\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "invert_sorted_idx=sorted_idx[::-1][:14]# keep only the 15 features with the highest average MDA\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.boxplot(result.importances[invert_sorted_idx].T,\n",
    "           vert=False, labels=data_test.columns[invert_sorted_idx])\n",
    "ax.set_title(\"MDA computed on the test set for the 15 features with the highest average MDA (average MDA is displayed in orange\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments about the plot above: ......."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 2: introduction to regression trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we consider the dataset named *srbct_data*. It is relative to small round blue cell tumors of childhood. This set is composed of :\n",
    "\n",
    "- a response factor of length 63, called class, indicating the class of each sample (4 classes in total).\n",
    "- 2308 predictors. Each predictor represents the expression of one gene. The features are correlated. \n",
    "\n",
    "More information about the data are available on https://www.rdocumentation.org/packages/plsgenomics/versions/1.5-2/topics/SRBCT \n",
    "\n",
    "The table named *genes_name* contains the names of the genes and a description for each gene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data \n",
    "import pandas as pd # data analysis\n",
    "srbct_data = pd.read_csv(\"cancer_data.csv\")\n",
    "genes_name = pd.read_csv(\"cancer_data_genes_names.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a look at the data \n",
    "srbct_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes_name.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the dimension and a brief statistics summary\n",
    "srbct_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a brief statistics summary\n",
    "srbct_data.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display a frequency table for the target variable named `class`.\n",
    "srbct_data[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the variables that we be used \n",
    "target_name = \"class\" # The response variable that we will consider\n",
    "features_names = list(srbct_data)\n",
    "features_names.remove('class') # The predictors/features used to predict the target\n",
    "#print(target_column)\n",
    "#print(features_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train (75% of data) and test dataset (25% of data)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y =  srbct_data[features_names], srbct_data[target_name]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0,test_size=0.25)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    " - 1) Build a random forest on this dataset by using the [RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor) class. Read carefully the documentation and use the default values for the RF parameters.\n",
    " - 2) Calibrate the two parameters `n_estimators` and `max_features` by using the same approach as in `Ex1 question 4`. Then, select values for `n_estimators` and `max_features`.\n",
    " - 3) Build a second random forest using the selected values for `n_estimators` and `max_features`.\n",
    " - 4) Because there are lots of features and they are correlated, use the MDA score to select a subset of only 20 variables (use the same approach as in `Ex1 question 9`). Justify your choice.\n",
    " - 5) Build a third random forest based only the selected subset of features.\n",
    " - 6) Compare the three models. What model do you select and why ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##---- Write your answer ----##\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
